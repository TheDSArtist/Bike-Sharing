{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "bb00408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import mlflow\n",
    "import json \n",
    "from sklearn.metrics import make_scorer\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c4167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "id": "55f24cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MLFLOW SETUP ---\n",
    "EXPERIMENT_NAME = \"Bike_Sharing_Demand_TSCV\"\n",
    "\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    # 'dteday',\n",
    "    'season',\n",
    "    'yr',\n",
    "    'mnth',\n",
    "    'holiday',\n",
    "    'weekday',\n",
    "    'workingday',\n",
    "    'weathersit',\n",
    "    'temp',\n",
    "    'atemp',\n",
    "    'hum',\n",
    "    'windspeed',\n",
    "    # 'casual',\n",
    "    # 'registered',\n",
    "    # 'bikes_cnt',\n",
    "    # 'day'\n",
    "    ] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "45128cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom function to calculate MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Mean Absolute Percentage Error (MAPE).\n",
    "    \n",
    "    Handles zero values in y_true by replacing them with a very small \n",
    "    epsilon value to avoid division by zero errors.\n",
    "    \"\"\"\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "    # Add a small epsilon to avoid division by zero\n",
    "    # np.where(y_true == 0, epsilon, y_true) ensures division is safe\n",
    "    epsilon = 1e-8 \n",
    "    \n",
    "    # Calculate |(Actual - Predicted) / Actual|\n",
    "    percentage_error = np.abs((y_true - y_pred) / np.where(y_true == 0, epsilon, y_true))\n",
    "    \n",
    "    # Calculate the mean and multiply by 100 to get a percentage\n",
    "    return np.mean(percentage_error) * 100\n",
    "\n",
    "def neg_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return -1.0 * mean_absolute_percentage_error(y_true, y_pred)\n",
    "\n",
    "mape_scorer = make_scorer(neg_mean_absolute_percentage_error, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955a90e",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "id": "715007be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONFIG\n",
    "\n",
    "FILE_PATH = \"data/dataset/day.csv\"\n",
    "TARGET_COL = 'cnt'\n",
    "MIN_TEST_SAMPLES = None #â‰ˆ # 210 #30\n",
    "MAX_TRAIN_SAMPLE = None # 360 #None # 180 # day\n",
    "\n",
    "SCORING_MARTIX = \"neg_mean_absolute_error\"\n",
    "# SCORING_MARTIX = 'neg_mean_squared_error'\n",
    "# SCORING_MARTIX = \"neg_root_mean_squared_error\"\n",
    "# SCORING_MARTIX = mape_scorer\n",
    "\n",
    "N_ITER_SEARCH = 15\n",
    "n_splits = 5\n",
    "# n_iter_search = 10\n",
    "\n",
    "# 1. Loading the data\n",
    "df_ = pd.read_csv(FILE_PATH, parse_dates=['dteday'])\n",
    "df_ = df_.sort_values(by='dteday').reset_index(drop=True)\n",
    "\n",
    "# split dataset\n",
    "df_last30 = df_.tail(30)\n",
    "df = df_.iloc[:-30, :]\n",
    "\n",
    "param_dist = {\n",
    "    'regressor__n_estimators': [200, 500, 800, 1000, 1500],\n",
    "    'regressor__max_depth': [15, 30, 45, 60, None], # Exploring deeper values\n",
    "    'regressor__min_samples_split': [2, 5, 10, 20, 40], # Testing higher regularization\n",
    "    'regressor__min_samples_leaf': [1, 3, 5, 10, 15],  # Testing higher regularization       \n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa6b2fb",
   "metadata": {},
   "source": [
    "# Feature Engg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "id": "bdfc5b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded with 701 rows and 29 columns.\n",
      "\n",
      " Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'holiday', 'weekday',\n",
      "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
      "       'casual', 'registered', 'cnt', 'is_weekend', 'lag_demand_1d',\n",
      "       'lag_demand_7d', 'temp_lag_1d', 'temp_lag_7d', 'atemp_lag_1d',\n",
      "       'atemp_lag_7d', 'hum_lag_1d', 'hum_lag_7d', 'windspeed_lag_1d',\n",
      "       'windspeed_lag_7d', 'temp_x_is_weekend', 'atemp_x_is_weekend'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['is_weekend'] = np.where(df['weekday'].isin([5, 6]), 1, 0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lag_demand_1d'] = df['cnt'].shift(1)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lag_demand_1d'] = df['lag_demand_1d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lag_demand_7d'] = df['cnt'].shift(1)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['lag_demand_7d'] = df['lag_demand_7d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['temp_lag_1d'] = df['temp'].shift(1)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['temp_lag_1d'] = df['temp_lag_1d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['temp_lag_7d'] = df['temp'].shift(7)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['temp_lag_7d'] = df['temp_lag_7d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['atemp_lag_1d'] = df['atemp'].shift(1)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['atemp_lag_1d'] = df['atemp_lag_1d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['atemp_lag_7d'] = df['atemp'].shift(7)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['atemp_lag_7d'] = df['atemp_lag_7d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hum_lag_1d'] = df['hum'].shift(1)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hum_lag_1d'] = df['hum_lag_1d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hum_lag_7d'] = df['hum'].shift(7)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hum_lag_7d'] = df['hum_lag_7d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['windspeed_lag_1d'] = df['windspeed'].shift(1)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['windspeed_lag_1d'] = df['windspeed_lag_1d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['windspeed_lag_7d'] = df['windspeed'].shift(7)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['windspeed_lag_7d'] = df['windspeed_lag_7d'].fillna(0)\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['temp_x_is_weekend'] = df['temp'] * df['is_weekend']\n",
      "/var/folders/_4/kyhrjqh97571rhyj62w1kdwr0000gn/T/ipykernel_82440/3371808845.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['atemp_x_is_weekend'] = df['atemp'] * df['is_weekend']\n"
     ]
    }
   ],
   "source": [
    "# 2. Feature engineering\n",
    "\n",
    "df['is_weekend'] = np.where(df['weekday'].isin([5, 6]), 1, 0)\n",
    "\n",
    "df['lag_demand_1d'] = df['cnt'].shift(1)\n",
    "df['lag_demand_1d'] = df['lag_demand_1d'].fillna(0) \n",
    "\n",
    "df['lag_demand_7d'] = df['cnt'].shift(1)\n",
    "df['lag_demand_7d'] = df['lag_demand_7d'].fillna(0) \n",
    "\n",
    "# --- Day of Week (7-day cycle) ---\n",
    "\n",
    "df['temp_lag_1d'] = df['temp'].shift(1)\n",
    "df['temp_lag_1d'] = df['temp_lag_1d'].fillna(0)\n",
    "\n",
    "df['temp_lag_7d'] = df['temp'].shift(7)\n",
    "df['temp_lag_7d'] = df['temp_lag_7d'].fillna(0)\n",
    "\n",
    "df['atemp_lag_1d'] = df['atemp'].shift(1)\n",
    "df['atemp_lag_1d'] = df['atemp_lag_1d'].fillna(0)\n",
    "\n",
    "df['atemp_lag_7d'] = df['atemp'].shift(7)\n",
    "df['atemp_lag_7d'] = df['atemp_lag_7d'].fillna(0)\n",
    "\n",
    "df['hum_lag_1d'] = df['hum'].shift(1)\n",
    "df['hum_lag_1d'] = df['hum_lag_1d'].fillna(0)\n",
    "\n",
    "df['hum_lag_7d'] = df['hum'].shift(7)\n",
    "df['hum_lag_7d'] = df['hum_lag_7d'].fillna(0)\n",
    "\n",
    "df['windspeed_lag_1d'] = df['windspeed'].shift(1)\n",
    "df['windspeed_lag_1d'] = df['windspeed_lag_1d'].fillna(0)\n",
    "\n",
    "df['windspeed_lag_7d'] = df['windspeed'].shift(7)\n",
    "df['windspeed_lag_7d'] = df['windspeed_lag_7d'].fillna(0)\n",
    "\n",
    "# Interaction: Temperature impact during peak usage hours\n",
    "\n",
    "# Interaction: Demand on weekends vs. workdays\n",
    "df['temp_x_is_weekend'] = df['temp'] * df['is_weekend']\n",
    "\n",
    "# Interaction: Demand on weekends vs. workdays\n",
    "df['atemp_x_is_weekend'] = df['atemp'] * df['is_weekend']\n",
    "\n",
    "FEATURE_COLS.extend([\n",
    "    'atemp_x_is_weekend', \n",
    "    'temp_x_is_weekend', \n",
    "    'windspeed_lag_7d', \n",
    "    'windspeed_lag_1d', \n",
    "    'hum_lag_7d', \n",
    "    'hum_lag_1d', \n",
    "    'atemp_lag_7d', \n",
    "    'atemp_lag_1d', \n",
    "    'temp_lag_7d', \n",
    "    'temp_lag_1d', \n",
    "    'is_weekend', \n",
    "    'lag_demand_7d', \n",
    "    'trend',\n",
    "    'yearly',\n",
    "    'weekly'\n",
    "    ]\n",
    "                    )\n",
    " \n",
    "print(f\"Data loaded with {len(df)} rows and {len(df.columns)} columns.\")\n",
    "print(\"\\n\", df.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "id": "ae564f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:46:44 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:46:44 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<prophet.forecaster.Prophet at 0x14e904a90>"
      ]
     },
     "execution_count": 1014,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Prophet(\n",
    "    growth='linear',\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=True # Set to True if your data is daily or finer\n",
    ")\n",
    "# 1. Prepare data for Prophet\n",
    "prophet_df = df[['dteday', 'cnt']].rename(columns={'dteday': 'ds', 'cnt': 'y'})\n",
    "m.fit(prophet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "id": "1ede486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Predict on the historical dataset to get component values\n",
    "future = m.make_future_dataframe(periods=0, freq='D')\n",
    "forecast = m.predict(future)\n",
    "\n",
    "# 3. Extract key Prophet components (components capture trend, seasonality, etc.)\n",
    "prophet_components = forecast[['ds', 'trend', 'yearly', 'weekly']]\n",
    "\n",
    "# 4. Merge these components back into your main training DataFrame (X)\n",
    "X_features = df.merge(prophet_components, \n",
    "                     left_on='dteday', \n",
    "                     right_on='ds', \n",
    "                     how='left')\n",
    "# Drop the redundant 'ds' column\n",
    "X_features = X_features.drop(columns=['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "id": "c6ea607b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['instant', 'dteday', 'season', 'yr', 'mnth', 'holiday', 'weekday',\n",
       "       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n",
       "       'casual', 'registered', 'cnt', 'is_weekend', 'lag_demand_1d',\n",
       "       'lag_demand_7d', 'temp_lag_1d', 'temp_lag_7d', 'atemp_lag_1d',\n",
       "       'atemp_lag_7d', 'hum_lag_1d', 'hum_lag_7d', 'windspeed_lag_1d',\n",
       "       'windspeed_lag_7d', 'temp_x_is_weekend', 'atemp_x_is_weekend', 'trend',\n",
       "       'yearly', 'weekly'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1016,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = X_features.copy()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "id": "aa0e54ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Identified Feature Types ---\n",
      "Numerical Features: ['temp', 'atemp', 'hum', 'windspeed', 'atemp_x_is_weekend', 'temp_x_is_weekend', 'windspeed_lag_7d', 'windspeed_lag_1d', 'hum_lag_7d', 'hum_lag_1d', 'atemp_lag_7d', 'atemp_lag_1d', 'temp_lag_7d', 'temp_lag_1d', 'lag_demand_7d', 'trend', 'yearly', 'weekly']\n",
      "Categorical Features: ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit', 'is_weekend']\n",
      "\n",
      "--- Starting Expanding Window Cross-Validation with Tuning ---\n",
      "\n",
      "[Fold 1/5] Training size: 121, Testing size: 116\n",
      "  Tuning Random Forest on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}\n",
      "  Fold Metrics: RMSE=643.21, MAE=528.10\n",
      "\n",
      "[Fold 2/5] Training size: 237, Testing size: 116\n",
      "  Tuning Random Forest on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}\n",
      "  Fold Metrics: RMSE=1013.59, MAE=862.32\n",
      "\n",
      "[Fold 3/5] Training size: 353, Testing size: 116\n",
      "  Tuning Random Forest on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}\n",
      "  Fold Metrics: RMSE=1268.25, MAE=991.96\n",
      "\n",
      "[Fold 4/5] Training size: 469, Testing size: 116\n",
      "  Tuning Random Forest on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}\n",
      "  Fold Metrics: RMSE=1127.92, MAE=942.81\n",
      "\n",
      "[Fold 5/5] Training size: 585, Testing size: 116\n",
      "  Tuning Random Forest on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}\n",
      "  Fold Metrics: RMSE=993.64, MAE=684.29\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Average RMSE over 5 folds: 1009.32\n",
      "Average MAE over 5 folds: 801.90\n",
      "Average MSE over 5 folds: 1061809.48\n",
      "Average r2 over 5 folds: 0.14\n",
      "Average MAPE over 5 folds: 54.11%\n",
      "avg_MAPE 54.114398971397506 avg_RMSE 1009.320274928184 avg_MAE 801.8954630436922 avg_r2 0.13859673065434627 avg_mse 1061809.4764382443 individual_folds [{'RMSE': np.float64(643.2072383377696), 'MAE': 528.098511199618, 'MSE': 413715.5514501004, 'r2': -0.25424704400127673, 'MAPE': np.float64(11.214564648456957), 'Best_Params': {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}}, {'RMSE': np.float64(1013.5937267873637), 'MAE': 862.3199529990137, 'MSE': 1027372.2429826969, 'r2': 0.0250737034752343, 'MAPE': np.float64(29.7886766508448), 'Best_Params': {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}}, {'RMSE': np.float64(1268.2457561512808), 'MAE': 991.9564535463605, 'MSE': 1608447.297995734, 'r2': 0.33724938077144584, 'MAPE': np.float64(29.283515634074604), 'Best_Params': {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}}, {'RMSE': np.float64(1127.9160493257393), 'MAE': 942.808100416463, 'MSE': 1272194.6143265837, 'r2': -0.0014199664585357041, 'MAPE': np.float64(18.052407931118537), 'Best_Params': {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}}, {'RMSE': np.float64(993.6386040387658), 'MAE': 684.2942970570055, 'MSE': 987317.6754361073, 'r2': 0.5863275794848637, 'MAPE': np.float64(182.23282999249267), 'Best_Params': {'regressor__n_estimators': 800, 'regressor__min_samples_split': 2, 'regressor__min_samples_leaf': 3, 'regressor__max_depth': None}}]\n"
     ]
    }
   ],
   "source": [
    "NUMERICAL_FEATURES = []\n",
    "CATEGORICAL_FEATURES = []\n",
    "\n",
    "# Feature Type Identification\n",
    "for col in FEATURE_COLS:\n",
    "    col_dtype = df[col].dtype\n",
    "    num_unique = df[col].nunique()\n",
    "    \n",
    "    if np.issubdtype(col_dtype, np.number) and 'float' in str(col_dtype):\n",
    "        NUMERICAL_FEATURES.append(col)\n",
    "    elif np.issubdtype(col_dtype, np.number) and num_unique <= 50:\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif np.issubdtype(col_dtype, np.number):\n",
    "            NUMERICAL_FEATURES.append(col)\n",
    "    elif col_dtype == 'object':\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "        \n",
    "print(\"\\n--- Identified Feature Types ---\")\n",
    "print(f\"Numerical Features: {NUMERICAL_FEATURES}\")\n",
    "print(f\"Categorical Features: {CATEGORICAL_FEATURES}\")\n",
    "\n",
    "X = df[FEATURE_COLS]\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# 2a. Create pre-processing pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', NUMERICAL_FEATURES), \n",
    "        ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(\n",
    "        n_splits=n_splits, \n",
    "        max_train_size=MAX_TRAIN_SAMPLE, \n",
    "        test_size= MIN_TEST_SAMPLES\n",
    "    )\n",
    "    \n",
    "cv_metrics = []\n",
    "\n",
    "print(\"\\n--- Starting Expanding Window Cross-Validation with Tuning ---\")\n",
    "\n",
    "# Inner split for tuning (used inside RandomizedSearchCV)\n",
    "inner_cv = TimeSeriesSplit(n_splits=3,\n",
    "                           test_size=MIN_TEST_SAMPLES)\n",
    "\n",
    "# --- START MLFLOW RUN ---\n",
    "# This context manager automatically starts and ends a run\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # 1. Log the featured columns as a **parameter**\n",
    "    mlflow.log_param(\"featured_columns\", json.dumps(FEATURE_COLS)) \n",
    "    \n",
    "    # Also log other parameters that define the experiment\n",
    "    mlflow.log_param(\"FILE_PATH\", FILE_PATH)\n",
    "    mlflow.log_param(\"n_splits\", n_splits)\n",
    "    mlflow.log_param(\"n_iter_search\", n_iter_search)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "    mlflow.log_param(\"SCORING_MARTIX\", SCORING_MARTIX)   \n",
    "    \n",
    "    N_ITER_SEARCH = 10 \n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        \n",
    "        # Prepare Data for Current Fold\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        print(f\"\\n[Fold {fold + 1}/{n_splits}] Training size: {len(X_train_fold)}, Testing size: {len(X_test_fold)}\")\n",
    "\n",
    "        # Define the Full ML Pipeline\n",
    "        full_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "        ])\n",
    "        \n",
    "        # Perform Randomized Search on the CURRENT Training Data\n",
    "        random_search = RandomizedSearchCV(\n",
    "            full_pipeline, \n",
    "            param_distributions=param_dist, \n",
    "            n_iter=n_iter_search, \n",
    "            scoring= SCORING_MARTIX, \n",
    "            cv=inner_cv, \n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        print(f\"  Tuning Random Forest on current training window (n_iter={n_iter_search})...\")\n",
    "        random_search.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Use the Best Model found to predict on the outer test fold\n",
    "        best_model = random_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_fold)\n",
    "        \n",
    "        # Evaluate Metrics for this Fold\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_fold, y_pred))\n",
    "        mae = mean_absolute_error(y_test_fold, y_pred)\n",
    "        mse_rf = mean_squared_error(y_test_fold, y_pred)\n",
    "        r2_rf = r2_score(y_test_fold, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test_fold, y_pred)\n",
    "        \n",
    "        # 2. Log Fold Metrics as **metrics**\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_rmse\", rmse)\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_mae\", mae)\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_r2\", r2_rf)\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_mse\", mse_rf)\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_mape\", mape)\n",
    "        \n",
    "        # 3. Log Best Parameters for the Fold as **parameters**\n",
    "        # Note: You may want to simplify this or only log the final best parameters\n",
    "        # For simplicity, we log all best params for the fold as a parameter\n",
    "        fold_params = {f\"fold_{fold+1}_best_params\": json.dumps(random_search.best_params_)}\n",
    "        mlflow.log_params(fold_params) \n",
    "        \n",
    "        cv_metrics.append({'RMSE': rmse,\n",
    "                           'MAE': mae, \n",
    "                           'MSE': mse_rf,\n",
    "                           'r2':r2_rf,\n",
    "                           'MAPE': mape,\n",
    "                           'Best_Params': random_search.best_params_})\n",
    "        \n",
    "        print(f\"  Best Parameters: {random_search.best_params_}\")\n",
    "        print(f\"  Fold Metrics: RMSE={rmse:.2f}, MAE={mae:.2f}\")\n",
    "\n",
    "    # Calculate and Report Averages\n",
    "    avg_rmse = np.mean([m['RMSE'] for m in cv_metrics])\n",
    "    avg_mae = np.mean([m['MAE'] for m in cv_metrics])\n",
    "    avg_mse = np.mean([m['MSE'] for m in cv_metrics])\n",
    "    avg_r2 = np.mean([m['r2'] for m in cv_metrics])\n",
    "    avg_mape = np.mean([m['MAPE'] for m in cv_metrics])\n",
    "\n",
    "    # 4. Log the final average metrics\n",
    "    mlflow.log_metric(\"avg_rmse\", avg_rmse)\n",
    "    mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "    mlflow.log_metric(\"avg_mse\", avg_mse)\n",
    "    mlflow.log_metric(\"avg_r2\", avg_r2)\n",
    "    mlflow.log_metric(\"avg_mape\", avg_mape)\n",
    "\n",
    "    print(\"\\n--- Cross-Validation Summary ---\")\n",
    "    print(f\"Average RMSE over {n_splits} folds: {avg_rmse:.2f}\")\n",
    "    print(f\"Average MAE over {n_splits} folds: {avg_mae:.2f}\")\n",
    "    print(f\"Average MSE over {n_splits} folds: {avg_mse:.2f}\")\n",
    "    print(f\"Average r2 over {n_splits} folds: {avg_r2:.2f}\")\n",
    "    print(f\"Average MAPE over {n_splits} folds: {avg_mape:.2f}%\")\n",
    "\n",
    "    print('avg_MAPE', avg_mape,'avg_RMSE', avg_rmse, 'avg_MAE', avg_mae, 'avg_r2', avg_r2, 'avg_mse', avg_mse, 'individual_folds', cv_metrics)\n",
    "# The run is automatically ended here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "id": "a5621e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8714\n",
      "22\n",
      "(701, 32)\n"
     ]
    }
   ],
   "source": [
    "print(df['cnt'].max())\n",
    "print(df['cnt'].min())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef59271a",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "98947d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Identified Feature Types ---\n",
      "Numerical Features: ['temp', 'atemp', 'hum', 'windspeed', 'atemp_x_is_weekend', 'temp_x_is_weekend', 'windspeed_lag_7d', 'windspeed_lag_1d', 'hum_lag_7d', 'hum_lag_1d', 'atemp_lag_7d', 'atemp_lag_1d', 'temp_lag_7d', 'temp_lag_1d', 'lag_demand_7d', 'trend', 'yearly', 'weekly']\n",
      "Categorical Features: ['season', 'yr', 'mnth', 'holiday', 'weekday', 'workingday', 'weathersit', 'is_weekend']\n",
      "\n",
      "--- Starting Expanding Window Cross-Validation with Tuning ---\n"
     ]
    }
   ],
   "source": [
    "param_dist_xgb = {\n",
    "    'regressor__n_estimators': [100, 300, 500],\n",
    "    'regressor__max_depth': [3, 6, 9, 12],\n",
    "    'regressor__learning_rate': [0.05, 0.1, 0.2],\n",
    "    'regressor__subsample': [0.7, 0.9],\n",
    "    'regressor__reg_alpha': [0, 0.1], # L1 regularization\n",
    "}\n",
    "\n",
    "\n",
    "NUMERICAL_FEATURES = []\n",
    "CATEGORICAL_FEATURES = []\n",
    "\n",
    "# Feature Type Identification\n",
    "for col in FEATURE_COLS:\n",
    "    col_dtype = df[col].dtype\n",
    "    num_unique = df[col].nunique()\n",
    "    \n",
    "    if np.issubdtype(col_dtype, np.number) and 'float' in str(col_dtype):\n",
    "        NUMERICAL_FEATURES.append(col)\n",
    "    elif np.issubdtype(col_dtype, np.number) and num_unique <= 50:\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "    elif np.issubdtype(col_dtype, np.number):\n",
    "            NUMERICAL_FEATURES.append(col)\n",
    "    elif col_dtype == 'object':\n",
    "        CATEGORICAL_FEATURES.append(col)\n",
    "        \n",
    "print(\"\\n--- Identified Feature Types ---\")\n",
    "print(f\"Numerical Features: {NUMERICAL_FEATURES}\")\n",
    "print(f\"Categorical Features: {CATEGORICAL_FEATURES}\")\n",
    "\n",
    "X = df[FEATURE_COLS]\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "# 2a. Create pre-processing pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', NUMERICAL_FEATURES), \n",
    "        ('cat', categorical_transformer, CATEGORICAL_FEATURES)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")\n",
    "\n",
    "tscv = TimeSeriesSplit(\n",
    "        n_splits=n_splits, \n",
    "        max_train_size=MAX_TRAIN_SAMPLE, \n",
    "        test_size= MIN_TEST_SAMPLES\n",
    "    )\n",
    "    \n",
    "cv_metrics = []\n",
    "\n",
    "print(\"\\n--- Starting Expanding Window Cross-Validation with Tuning ---\")\n",
    "\n",
    "# Inner split for tuning (used inside RandomizedSearchCV)\n",
    "inner_cv = TimeSeriesSplit(n_splits=3,\n",
    "                           test_size=MIN_TEST_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "72113179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fold 1/5] Training size: 121, Testing size: 116\n",
      "  Tuning XGBoost on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__subsample': 0.7, 'regressor__reg_alpha': 0, 'regressor__n_estimators': 100, 'regressor__max_depth': 12, 'regressor__learning_rate': 0.2}\n",
      "  Fold Metrics: RMSE=581.92, MAE=467.85, MAPE=9.98%\n",
      "\n",
      "[Fold 2/5] Training size: 237, Testing size: 116\n",
      "  Tuning XGBoost on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__subsample': 0.9, 'regressor__reg_alpha': 0.1, 'regressor__n_estimators': 300, 'regressor__max_depth': 6, 'regressor__learning_rate': 0.05}\n",
      "  Fold Metrics: RMSE=852.40, MAE=663.93, MAPE=25.57%\n",
      "\n",
      "[Fold 3/5] Training size: 353, Testing size: 116\n",
      "  Tuning XGBoost on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__subsample': 0.9, 'regressor__reg_alpha': 0, 'regressor__n_estimators': 500, 'regressor__max_depth': 6, 'regressor__learning_rate': 0.2}\n",
      "  Fold Metrics: RMSE=1224.46, MAE=929.35, MAPE=26.74%\n",
      "\n",
      "[Fold 4/5] Training size: 469, Testing size: 116\n",
      "  Tuning XGBoost on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__subsample': 0.7, 'regressor__reg_alpha': 0, 'regressor__n_estimators': 500, 'regressor__max_depth': 3, 'regressor__learning_rate': 0.1}\n",
      "  Fold Metrics: RMSE=908.47, MAE=727.73, MAPE=13.54%\n",
      "\n",
      "[Fold 5/5] Training size: 585, Testing size: 116\n",
      "  Tuning XGBoost on current training window (n_iter=10)...\n",
      "  Best Parameters: {'regressor__subsample': 0.9, 'regressor__reg_alpha': 0, 'regressor__n_estimators': 100, 'regressor__max_depth': 3, 'regressor__learning_rate': 0.2}\n",
      "  Fold Metrics: RMSE=876.49, MAE=610.61, MAPE=145.46%\n",
      "\n",
      "--- Cross-Validation Summary ---\n",
      "Average RMSE over 5 folds: 888.75\n",
      "Average MAE over 5 folds: 679.89\n",
      "Average MAPE over 5 folds: 44.26%\n",
      "Average MSE over 5 folds: 831613.43\n",
      "Average r2 over 5 folds: 0.34\n",
      "avg_RMSE 888.7472818052286 avg_MAE 679.89287109375 avg_MAPE 44.25801715080454 avg_r2 0.338918137550354 avg_mse 831613.43125\n"
     ]
    }
   ],
   "source": [
    "# --- 4. MLFLOW RUN LOOP ---\n",
    "with mlflow.start_run():\n",
    "    \n",
    "    # Log initial parameters\n",
    "    mlflow.log_param(\"featured_columns\", json.dumps(FEATURE_COLS)) \n",
    "    mlflow.log_param(\"n_splits\", n_splits)\n",
    "    mlflow.log_param(\"n_iter_search\", N_ITER_SEARCH)\n",
    "    mlflow.log_param(\"model_type\", \"XGBoostRegressor\")\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "        \n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        print(f\"\\n[Fold {fold + 1}/{n_splits}] Training size: {len(X_train_fold)}, Testing size: {len(X_test_fold)}\")\n",
    "\n",
    "        # Define the Full ML Pipeline with XGBoost\n",
    "        full_pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', XGBRegressor(random_state=42, n_jobs=-1, eval_metric='rmse', verbosity=0)) \n",
    "        ])\n",
    "        \n",
    "        # Perform Randomized Search on the CURRENT Training Data\n",
    "        random_search = RandomizedSearchCV(\n",
    "            full_pipeline, \n",
    "            param_distributions=param_dist_xgb, \n",
    "            n_iter=N_ITER_SEARCH, \n",
    "            scoring=mape_scorer, # Optimized scoring using NEGATED MAPE\n",
    "            cv=inner_cv, \n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        print(f\"  Tuning XGBoost on current training window (n_iter={N_ITER_SEARCH})...\")\n",
    "        random_search.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        best_model = random_search.best_estimator_\n",
    "        y_pred = best_model.predict(X_test_fold)\n",
    "        \n",
    "        # --- CALCULATE ALL METRICS ---\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_fold, y_pred))\n",
    "        mae = mean_absolute_error(y_test_fold, y_pred)\n",
    "        mse_rf = mean_squared_error(y_test_fold, y_pred)\n",
    "        r2_rf = r2_score(y_test_fold, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test_fold, y_pred) # Positive MAPE\n",
    "        \n",
    "        # Log Fold Metrics \n",
    "        mlflow.log_metric(f\"fold_{fold+1}_rmse\", rmse)\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_mae\", mae)\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_r2\", r2_rf)\n",
    "        mlflow.log_metric(f\"fold_{fold+1}_mape\", mape) # Log the positive value\n",
    "        \n",
    "        # Log Best Parameters\n",
    "        fold_params = {f\"fold_{fold+1}_best_params\": json.dumps(random_search.best_params_)}\n",
    "        mlflow.log_params(fold_params) \n",
    "        \n",
    "        cv_metrics.append({'RMSE': rmse,\n",
    "                           'MAE': mae, \n",
    "                           'MSE': mse_rf,\n",
    "                           'r2':r2_rf,\n",
    "                           'MAPE': mape,\n",
    "                           'Best_Params': random_search.best_params_})\n",
    "        \n",
    "        print(f\"  Best Parameters: {random_search.best_params_}\")\n",
    "        print(f\"  Fold Metrics: RMSE={rmse:.2f}, MAE={mae:.2f}, MAPE={mape:.2f}%\")\n",
    "\n",
    "    # --- 5. LOG FINAL AVERAGE METRICS ---\n",
    "    \n",
    "    avg_rmse = np.mean([m['RMSE'] for m in cv_metrics])\n",
    "    avg_mae = np.mean([m['MAE'] for m in cv_metrics])\n",
    "    avg_mse = np.mean([m['MSE'] for m in cv_metrics])\n",
    "    avg_r2 = np.mean([m['r2'] for m in cv_metrics])\n",
    "    avg_mape = np.mean([m['MAPE'] for m in cv_metrics])\n",
    "\n",
    "    mlflow.log_metric(\"avg_rmse\", avg_rmse)\n",
    "    mlflow.log_metric(\"avg_mae\", avg_mae)\n",
    "    mlflow.log_metric(\"avg_mse\", avg_mse)\n",
    "    mlflow.log_metric(\"avg_r2\", avg_r2)\n",
    "    mlflow.log_metric(\"avg_mape\", avg_mape)\n",
    "\n",
    "    print(\"\\n--- Cross-Validation Summary ---\")\n",
    "    print(f\"Average RMSE over {n_splits} folds: {avg_rmse:.2f}\")\n",
    "    print(f\"Average MAE over {n_splits} folds: {avg_mae:.2f}\")\n",
    "    print(f\"Average MAPE over {n_splits} folds: {avg_mape:.2f}%\")\n",
    "    print(f\"Average MSE over {n_splits} folds: {avg_mse:.2f}\")\n",
    "    print(f\"Average r2 over {n_splits} folds: {avg_r2:.2f}\")\n",
    "\n",
    "    print('avg_RMSE', avg_rmse, 'avg_MAE', avg_mae, 'avg_MAPE', avg_mape, 'avg_r2', avg_r2, 'avg_mse', avg_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad7fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af731b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b72e0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5cc7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c97843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231d213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "612ded66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "69e528b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1        0        6           0   \n",
       "1        2  2011-01-02       1   0     1        0        0           0   \n",
       "2        3  2011-01-03       1   0     1        0        1           1   \n",
       "3        4  2011-01-04       1   0     1        0        2           1   \n",
       "4        5  2011-01-05       1   0     1        0        3           1   \n",
       "\n",
       "   weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0           2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1           2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2           1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3           1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4           1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "\n",
       "    cnt  \n",
       "0   985  \n",
       "1   801  \n",
       "2  1349  \n",
       "3  1562  \n",
       "4  1600  "
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/dataset/day.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "124f5fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 731 entries, 0 to 730\n",
      "Data columns (total 16 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     731 non-null    int64  \n",
      " 1   dteday      731 non-null    object \n",
      " 2   season      731 non-null    int64  \n",
      " 3   yr          731 non-null    int64  \n",
      " 4   mnth        731 non-null    int64  \n",
      " 5   holiday     731 non-null    int64  \n",
      " 6   weekday     731 non-null    int64  \n",
      " 7   workingday  731 non-null    int64  \n",
      " 8   weathersit  731 non-null    int64  \n",
      " 9   temp        731 non-null    float64\n",
      " 10  atemp       731 non-null    float64\n",
      " 11  hum         731 non-null    float64\n",
      " 12  windspeed   731 non-null    float64\n",
      " 13  casual      731 non-null    int64  \n",
      " 14  registered  731 non-null    int64  \n",
      " 15  cnt         731 non-null    int64  \n",
      "dtypes: float64(4), int64(11), object(1)\n",
      "memory usage: 91.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "b9605a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(731, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "      <th>season_2</th>\n",
       "      <th>season_3</th>\n",
       "      <th>season_4</th>\n",
       "      <th>yr_1</th>\n",
       "      <th>holiday_1</th>\n",
       "      <th>...</th>\n",
       "      <th>mnth_11</th>\n",
       "      <th>mnth_12</th>\n",
       "      <th>weekday_1</th>\n",
       "      <th>weekday_2</th>\n",
       "      <th>weekday_3</th>\n",
       "      <th>weekday_4</th>\n",
       "      <th>weekday_5</th>\n",
       "      <th>weekday_6</th>\n",
       "      <th>weathersit_2</th>\n",
       "      <th>weathersit_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       temp     atemp       hum  windspeed   cnt  season_2  season_3  \\\n",
       "0  0.344167  0.363625  0.805833   0.160446   985     False     False   \n",
       "1  0.363478  0.353739  0.696087   0.248539   801     False     False   \n",
       "2  0.196364  0.189405  0.437273   0.248309  1349     False     False   \n",
       "3  0.200000  0.212122  0.590435   0.160296  1562     False     False   \n",
       "4  0.226957  0.229270  0.436957   0.186900  1600     False     False   \n",
       "\n",
       "   season_4   yr_1  holiday_1  ...  mnth_11  mnth_12  weekday_1  weekday_2  \\\n",
       "0     False  False      False  ...    False    False      False      False   \n",
       "1     False  False      False  ...    False    False      False      False   \n",
       "2     False  False      False  ...    False    False       True      False   \n",
       "3     False  False      False  ...    False    False      False       True   \n",
       "4     False  False      False  ...    False    False      False      False   \n",
       "\n",
       "   weekday_3  weekday_4  weekday_5  weekday_6  weathersit_2  weathersit_3  \n",
       "0      False      False      False       True          True         False  \n",
       "1      False      False      False      False          True         False  \n",
       "2      False      False      False      False         False         False  \n",
       "3      False      False      False      False         False         False  \n",
       "4       True      False      False      False         False         False  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 1024,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(columns=['instant', 'dteday', 'casual', 'registered'])  # Dropping unnecessary columns\n",
    "\n",
    "# Apply One-Hot Encoding to categorical features\n",
    "df_encoded = pd.get_dummies(df, columns=['season', 'yr', 'holiday', 'workingday', 'mnth', 'weekday', 'weathersit'], drop_first=True)\n",
    "\n",
    "print(df_encoded.shape)\n",
    "\n",
    "# Display the first few rows of the encoded dataset\n",
    "df_encoded.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "id": "ca104e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_encoded.drop(columns=['cnt'])  # Features\n",
    "y = df_encoded['cnt']  # Target variable\n",
    "\n",
    "# Split data (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "d7cc5adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science-task",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
